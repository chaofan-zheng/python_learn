import torch
import numpy as np
from matplotlib import pyplot as plt
import math


def test1():
    """
    æŒ‡å®šä¸€ä¸ªå‡½æ•°ï¼Œè‡ªåŠ¨è®¡ç®—æ¢¯åº¦
    :return:
    """
    x = torch.arange(4.0)
    print(x)

    x.requires_grad_(True)  # ç­‰ä»·äºx=torch.arange(4.0,requires_grad=True) è¡¨ç¤ºéœ€è¦æ±‚å¯¼
    print(x.grad)  # é»˜è®¤æ˜¯None

    y = 2 * torch.dot(x, x)  # å‡è®¾æˆ‘ä»¬æƒ³å¯¹å‡½æ•° ğ‘¦=2ğ±âŠ¤ğ± å…³äºåˆ—å‘é‡ ğ± æ±‚å¯¼ã€‚
    print(y)  # tensor(28., grad_fn=<MulBackward0>)

    # åå‘ä¼ æ’­å‡½æ•°æ¥è‡ªåŠ¨è®¡ç®—yå…³äºxæ¯ä¸ªåˆ†é‡çš„æ¢¯åº¦ï¼Œå¹¶æ‰“å°è¿™äº›æ¢¯åº¦ã€‚
    y.backward()
    print(x.grad)
    # éªŒè¯æ¢¯åº¦è®¡ç®—æ˜¯å¦æ­£ç¡®
    print(x.grad == 4 * x)

    # åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼ŒPyTorchä¼šç´¯ç§¯æ¢¯åº¦ï¼Œæˆ‘ä»¬éœ€è¦æ¸…é™¤ä¹‹å‰çš„å€¼
    x.grad.zero_()


def test2():
    x = torch.arange(4.0)
    x.requires_grad_(True)
    y = x.sum()
    y.backward()
    print(x.grad)


def test3():
    """
    éæ ‡é‡å˜é‡çš„åå‘ä¼ æ’­
    :return:
    """
    x = torch.arange(4.0)
    x.requires_grad_(True)
    # å¯¹éæ ‡é‡è°ƒç”¨backwardéœ€è¦ä¼ å…¥ä¸€ä¸ªgradientå‚æ•°ï¼Œè¯¥å‚æ•°æŒ‡å®šå¾®åˆ†å‡½æ•°å…³äºselfçš„æ¢¯åº¦ã€‚
    # åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åªæƒ³æ±‚åå¯¼æ•°çš„å’Œï¼Œæ‰€ä»¥ä¼ é€’ä¸€ä¸ª1çš„æ¢¯åº¦æ˜¯åˆé€‚çš„
    y = x * x
    # ç­‰ä»·äºy.backward(torch.ones(len(x)))
    y.sum().backward()
    print(x.grad)


def test4():
    """
    åˆ†ç¦»è®¡ç®—
    æ±‚ zå…³äºxçš„æ¢¯åº¦ï¼Œä½†ç”±äºæŸç§åŸå› ï¼Œæˆ‘ä»¬å¸Œæœ›å°†yè§†ä¸ºä¸€ä¸ªå¸¸æ•°ï¼Œ å¹¶ä¸”åªè€ƒè™‘åˆ°xåœ¨yè¢«è®¡ç®—åå‘æŒ¥çš„ä½œç”¨ã€‚
    æ­¤æ—¶éœ€è¦æŠŠyè§†ä¸ºä¸€ä¸ªå¸¸æ•°u
    :return:
    """
    x = torch.arange(4.0, requires_grad=True)
    y = x * x
    u = y.detach()  # ä½†ä¸¢å¼ƒè®¡ç®—å›¾ä¸­å¦‚ä½•è®¡ç®—yçš„ä»»ä½•ä¿¡æ¯
    z = u * x

    z.sum().backward()
    #
    print(x.grad == u)

    x.grad.zero_()
    y.sum().backward()  # ç”±äºè®°å½•äº†yçš„è®¡ç®—ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥éšååœ¨yä¸Šè°ƒç”¨åå‘ä¼ æ’­ï¼Œ å¾—åˆ°y=x*xå…³äºçš„xçš„å¯¼æ•°ï¼Œå³2*xã€‚
    print(x.grad == 2 * x)


def f(a):
    """
    python æ§åˆ¶æµçš„æ¢¯åº¦è®¡ç®—
    :param a:
    :return:
    """
    b = a * 2
    while b.norm() < 1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c


def test5():
    a = torch.randn(size=(), requires_grad=True)  # éšæœºæ•°
    d = f(a)
    d.backward()
    print(a.grad == d / a)


def test6():
    a = torch.randn(size=(3, 4), requires_grad=True)  # éšæœºæ•° 3ï¼Œ4çš„çŸ©é˜µ
    d = f(a)
    d.sum().backward()
    print(a.grad == d / a)


def f2(x: torch.tensor):
    # y = torch.tensor([math.sin(i) for i in x])
    y = torch.sin(x)
    return y


def test7():
    """
    ä½¿ ğ‘“(ğ‘¥)=sin(ğ‘¥) ï¼Œç»˜åˆ¶ ğ‘“(ğ‘¥) å’Œ ğ‘‘ğ‘“(ğ‘¥)ğ‘‘ğ‘¥ çš„å›¾åƒï¼Œå…¶ä¸­åè€…ä¸ä½¿ç”¨ ğ‘“â€²(ğ‘¥)=cos(ğ‘¥) ã€‚
    :return:
    """
    x = np.linspace(-3 * np.pi, 3 * np.pi, 100)
    x1 = torch.tensor(x, requires_grad=True)
    # å¸¦æœ‰requires_grad=True éœ€è¦æ±‚å¯¼çš„å¼ é‡ä¸èƒ½å¤Ÿç”»å›¾
    y1 = f2(x1)
    print(y1)
    y1.sum().backward()

    plt.plot(x, y1.detach().numpy(), label="sinx") # å¸¦æœ‰requires_grad=True éœ€è¦æ±‚å¯¼çš„å¼ é‡ä¸èƒ½å¤Ÿç”»å›¾
    plt.plot(x, x1.grad, label="cosx")
    plt.legend()
    plt.show()


if __name__ == '__main__':
    # test1()
    test7()
