# why Pytorch?
 - 加速深度学习，利用GPU加速训练
 - 采用动态计算图
 - 自动反微分技术，零延迟改变网络行为。

主要构成
 - torch 类似于numpy，将张量转化为torch.cuda.TensorFloat 在GPU上进行计算。(numpy 在cpu进行计算)
 - torch.autograd 自动获取梯度
 - torch.nn 具有共享层和损失函数的神经网络
 - torch.optim 具有通用优化算法(SGD、Adam等)
 
 # 机器学习
 ##一般流程
 ### 分类
 - 监督学习 分类、回归、目标检测、识别
 - 无监督学习 聚类
 - 半监督学习 自编码、生成式对抗
 - 强化学习 奖惩
 
 一般流程： 清洗探索与清洗 -> 训练集训练模型 -> 测试集测试模型 -> 模型评估以及优化 -> 部署
 
 ### 数据清洗：
 - 缺失数据、
 - 数据不规范（格式转换）、
 - 数据分布不均衡（使用不同的降采样频率进行降采样）、
 - 存在非数值型数据、
 - 存在无关或者不重要的数据（删除无人片段、删除模糊、大角度的图片）。
 
 ### 选择模型以及对应的损失函数
 一般不存在任何情况都表现得很好的算法，（No Free Lunch）,一般会选用几种不同的方法来训练模型，随后比较性能，选择最优的那一个。
 
 ### 模型评估以及优化模型
 - 留出法
 把数据集直接拆成三个互斥的集合 训练集、验证集、测试集。
 - K折交叉验证。
 不重复的随机的将训练数据集划分为k个，k-1个用于训练模型，1个用于测试，一共k次。
 
 ## 过拟合与欠拟合
 欠拟合：测试精度和预测精度都不高，模型有能力通过迭代次数、优化方式进行提高精度。
 过拟合：测试精度很高、预测精度反而上升。模型过于复杂或者训练数据样本太少。 
 